const PORT = Number(process.env.PORT) || 8080
import * as lb from "@google-cloud/logging-bunyan"
import express from "express"
import fs from "fs"
import zlib from "zlib"
import Np24Scraper from "./datasource/incident/np24"
import {
  // EmptyArticleRepository,
  FirestoreArticleRepository,
  ArticleRepository,
  IndexScraper,
} from "./datasource/incident"
import axios from "axios"
import { ArticleScrapers } from "./datasource/incident/scraper"
import YahooIndexScraper from "./datasource/incident/yahoo"
import { IndexImporter } from "./datasource/mountain/gsi/convert2db"
import * as admin from "firebase-admin"

async function startServer() {
  const app = express()
  const { logger, mw } = await lb.express.middleware({
    logName: "yomoyama-server",
  })
  app.use(mw)

  admin.initializeApp({
    credential: admin.credential.applicationDefault(),
  })
  const firestore = admin.firestore()

  const repository: ArticleRepository = new FirestoreArticleRepository(
    firestore
  )

  async function update(
    repository: ArticleRepository,
    indexScraper: IndexScraper
  ): Promise<void> {
    logger.info("updateing " + indexScraper.constructor.name)
    const articleScrapers = new ArticleScrapers()
    const articleUrls = await indexScraper.getArticleUrls()

    const articlePromise = articleUrls.map(url => {
      return articleScrapers.scrape(url)
    })
    const allPromise = await Promise.all(articlePromise)
    const allArticle = allPromise
      .reduce((acc, curr) => acc.concat(curr), [])
      .filter(a => a)

    logger.info(`extract ${allArticle.length} articles.`)
    if (allArticle.length == 0) {
      logger.error(`article extraction result is 0. something went wrong?`)
    }

    Promise.all(
      allArticle.map(article => {
        logger.info(`${article.url} ${article.toKey().getId()}`)
        return repository.save(article)
      })
    )
  }

  app.get("/", (req, res) => {
    res.send("ðŸŽ‰ Hello TypeScript! ðŸŽ‰")
  })

  app.get("/datasource/mountain/incident/np24/update", async (req, res) => {
    if (process.env.NODE_ENV !== "development") {
      if (req.header("X-Appengine-Cron") !== "true") {
        res.send("NG")
        res.status(401)
        return
      }
    }
    req.log.info("updateing np24")
    const indexScraper = new Np24Scraper()
    await update(repository, indexScraper)
    res.send("OK")
  })

  app.get("/datasource/mountain/incident/yahoo/update", async (req, res) => {
    if (process.env.NODE_ENV !== "development") {
      if (req.header("X-Appengine-Cron") !== "true") {
        res.send("NG")
        res.status(401)
        return
      }
    }
    req.log.info("updateing yahoo")
    const indexScraper = new YahooIndexScraper()
    await update(repository, indexScraper)
    res.send("OK")
  })

  app.get("/datasource/mountain/incident/modify", async (req, res) => {
    req.log.info("updateing...")
    const articles = await repository.findAll("yj-news")
    req.log.info(`loaded ${articles.length} articles`)
    const modifiedArticles = articles
      .filter(a => a.tags.has("å±±å²³äº‹æ•…"))
      .filter(a => {
        if (
          // (a.content + a.subject).match(/é­é›£/)
          a.content.match(
            /(æŒ‡åå¼|è¿½æ‚¼|æŒ‡å®šå¼|ç™ºéšŠå¼|é–‹å§‹å¼|ç¥ˆé¡˜|è¨“ç·´ã‚’|é–‹è¨­|ä¼šè­°|ãƒ¯ãƒ‹|æ”¿åºœ|åœ°éœ‡|ãƒ¨ãƒƒãƒˆ|è¨“ç¤º|æ³¨æ„ç‚¹|ç´„æŸ|è¦³å…‰|æ„Ÿè¬çŠ¶|æ€ã„ã‚„ã‚Š|æ€¥å¢—ã—ã¦ã„ã‚‹|å‘½å|æ¿€æ’®|è¨­ç½®|ãƒªãƒ‹ã‚¢|å ±å‘Š|å‡ºç™ºå¼|ç¤¾ä¼šè²¢çŒ®|é–‰æ‰€å¼|æ°—ãŒã‹ã‚Š|çµéšŠå¼|ç¥ˆã‚Š|ç¥ˆã‚‹|çŒ®èŠ±|è¿½æ‚¼å¼|é–‹å¹•|å†¥ç¦|æŽˆæ¥­|éºæ—|æ…°éœŠ)/
          ) ||
          a.subject.match(
            /(æŒ‡åå¼|è¿½æ‚¼|æŒ‡å®šå¼|ç™ºéšŠå¼|é–‹å§‹å¼|ç¥ˆé¡˜|è¨“ç·´ã‚’|é–‹è¨­|ä¼šè­°|ãƒ¯ãƒ‹|æ”¿åºœ|åœ°éœ‡|ãƒ¨ãƒƒãƒˆ|è¨“ç¤º|æ³¨æ„ç‚¹|ç´„æŸ|è¦³å…‰|æ„Ÿè¬çŠ¶|æ€ã„ã‚„ã‚Š|æ€¥å¢—ã—ã¦ã„ã‚‹|å‘½å|æ¿€æ’®|è¨­ç½®|ãƒªãƒ‹ã‚¢|å ±å‘Š|å‡ºç™ºå¼|ç¤¾ä¼šè²¢çŒ®|é–‰æ‰€å¼|æ°—ãŒã‹ã‚Š|çµéšŠå¼|ç¥ˆã‚Š|ç¥ˆã‚‹|çŒ®èŠ±|è¿½æ‚¼å¼|é–‹å¹•|å†¥ç¦|æŽˆæ¥­|éºæ—|æ…°éœŠ)/
          )
        ) {
          return true
        } else {
          return false
        }
      })
      // .filter(a => idList.includes(a.toKey().getId()))
      .map(a => {
        a.tags.delete("å±±å²³äº‹æ•…")
        req.log.info("deleted å±±å²³äº‹æ•… ", a.toKey().getId(), a.subject, a.url)
        return a
      })

    await Promise.all(
      modifiedArticles.map(article => {
        return repository.save(article)
      })
    )
    res.send(modifiedArticles.map(e => e.toData()))
  })

  app.get("/datasource/mountain/incident/hide", async (req, res) => {
    req.log.info("updateing...")
    const articles = await repository.findAll("yj-news")
    req.log.info(`loaded ${articles.length} articles`)

    const hiddenKeys: Array<String> = []
    const modifiedArticles = articles
      .filter(a => a.tags.has("å±±å²³äº‹æ•…") && !a.tags.has("hidden"))
      .filter(a => hiddenKeys.includes(a.toKey().getId()))
      // .filter(a => idList.includes(a.toKey().getId()))
      .map(a => {
        a.tags.add("hidden")
        req.log.info("add hidden ", a.toKey().getId(), a.subject, a.url)
        return a
      })

    await Promise.all(
      modifiedArticles.map(article => {
        return repository.save(article)
      })
    )
    res.send(modifiedArticles.map(e => e.toData()))
  })

  app.get("/datasource/mountain/incident/yahoo/show", async (req, res) => {
    const indexScraper = new YahooIndexScraper()
    const articleUrls = await indexScraper.getArticleUrls()

    const articleScrapers = new ArticleScrapers()
    const articlePromise = articleUrls.map(url => {
      return articleScrapers.scrape(url)
    })
    const allPromise = await Promise.all(articlePromise)
    const allArticle = allPromise
      .reduce((acc, curr) => acc.concat(curr), [])
      .filter(a => a)
      .map(a => a.toData())

    req.log.info(`extract ${allArticle.length} articles.`)

    res.send(allArticle)
  })

  async function notifyToNetlify(): Promise<void> {
    if (process.env.NETLIFY_HOOK_URL) {
      const hookUrl: string = process.env.NETLIFY_HOOK_URL
      await axios.post(hookUrl, {})
      logger.info(`nofity to netlify ${hookUrl}`)
    } else {
      logger.info(
        "netlify rebuild hook skipped. due to process.env.NETLIFY_HOOK_URL is empty."
      )
    }
    return
  }

  app.get("/generate", async (req, res) => {
    await notifyToNetlify()
    res.send("UPDATED")
  })

  app.get("/datasource/mountain/gsi/index", async (req, res) => {
    const stream = fs
      .createReadStream("data/mokuroku-experimental_nnfpt.csv.gz")
      .pipe(zlib.createGunzip())

    const importer = new IndexImporter("experimental_nnfpt", firestore)
    await importer.importIndex(stream)
    res.send("OK")
  })
  app.get("/datasource/mountain/gsi/update-raw", async (req, res) => {
    const importer = new IndexImporter("experimental_nnfpt", firestore)
    await importer.updateAllContent()
    req.log.info("test")
    res.send("OK")
  })
  app.get("/datasource/mountain/gsi/raw-to-test", async (req, res) => {
    const importer = new IndexImporter("experimental_nnfpt", firestore)
    await importer.updateAllMountain()
    req.log.info("test")
    res.send("OK")
  })

  app.listen(PORT, () => {
    logger.info(`App listening on port ${PORT}`)
  })
}

startServer()
